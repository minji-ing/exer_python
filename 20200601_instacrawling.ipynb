{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링에 필요한 모듈 import\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 사용할 웹드라이버 경로 지정\n",
    "driver = webdriver.Chrome('D:/webCrawling/chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인스타 접속하는 코드\n",
    "driver.get('https://www.instagram.com')\n",
    "# 링크 불러오는 걸 2초간 delay 설정. 이미지 많을 경우 로딩 시간 필요하기 때문\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['.지인통해 가봤지만 다수 TV출연한 통영해물보쌈나도 가봤지만 사람많은 이유를 알겠음!!고기도먹고 회도먹고블로그도 광고가아니라 실제 먹은 후기들 보니까 믿을만하네요.단체석80석 완비 단체손님도 문제 없을듯!!..통영해물보쌈02-585-4378.회식이나 모임장소는 꼭 문의하고 와야될듯해요!!..#사당맛집 #사당역맛집 #사당회식장소 #사당역회식장소 #사당모임장소 #사당역모임장소 #사당횟집', '2020-05-26', '1,571', ' ', ['#사당맛집', '#사당역맛집', '#사당회식장소', '#사당역회식장소', '#사당모임장소', '#사당역모임장소', '#사당횟집']], ['..사당 맛집 탐구생활 🔍사당~이수에서 먹어볼만한맛집들 모았다 📌@@이거 먹고싶어! 😚...#사당맛집 #사당족발 #사당역맛집 #이수맛집 #생생정보통맛집 #천하무족 #쁘띠보노 #육촌 #스시로로 #남성집 #티에리스 #로우슬로우', '2020-05-26', '1,087', ' ', ['#사당맛집', '#사당족발', '#사당역맛집', '#이수맛집', '#생생정보통맛집', '#천하무족', '#쁘띠보노', '#육촌', '#스시로로', '#남성집', '#티에리스', '#로우슬로우']]]\n"
    }
   ],
   "source": [
    "# 인스타 검색하기 위한 url 생성\n",
    "word = '사당술집' # 검색할 검색어 설정\n",
    "url = insta_searching(word) # insta_searching function 사용하여 url 생성\n",
    "\n",
    "# 로그인 후 생성한 url 접속\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "# select_first function 사용하여 첫번째 게시글 열기\n",
    "select_first(driver)\n",
    "\n",
    "# 비어있는 리스트 변수 선언\n",
    "result = [ ]\n",
    "\n",
    "# 몇 개 게시글 크롤링할지 설정\n",
    "target = 10\n",
    "\n",
    "# 설정한 target만큼 수집\n",
    "for i in range(target):\n",
    "    try:\n",
    "        data = get_content(driver) # 게시글 파싱해서 필요한 정보 수집\n",
    "        result.append(data) # 미리 설정한 리스트에 저장\n",
    "        move_next(driver) # 게시글 정보 저장 끝나면 다음 게시글로 넘어감\n",
    "    except:\n",
    "        # 페이지 오류가 발생했을 경우 2초 기다리고 다음 페이지로 이동\n",
    "        time.sleep(2)\n",
    "        move_next(driver)\n",
    "print(result[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 하는 데 사용할 function 정의\n",
    "def insta_searching(word):\n",
    "    url = 'https://www.instagram.com/explore/tags/' + word\n",
    "    return url\n",
    "\n",
    "# 첫번째 게시글 여는 function 정의\n",
    "def select_first(driver):\n",
    "    first = driver.find_element_by_css_selector('div._9AhH0')\n",
    "    first.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "# 페이지 파싱해서 필요한 정보만 가져오는 function 정의\n",
    "def get_content(driver):\n",
    "    html = driver.page_source # 접속한 페이지 정보 가져오기\n",
    "    soup = BeautifulSoup(html, 'lxml') # 가져온 정보 파싱\n",
    "    try:\n",
    "        # 특정 태그 안에 있는 text 정보 가져오기\n",
    "        content = soup.select('div.C4VMK > span')[0].text\n",
    "    except:\n",
    "        # 예외 처리\n",
    "        content = ' '\n",
    "    # content 정보 중 해시태그 정보 가져오기(정규식 사용)\n",
    "    tags = re.findall(r'#[^\\s#,\\\\]+', content)\n",
    "    # content 정보 중 작성일자 정보 가져오기\n",
    "    date = soup.select('time._1o9PC.Nzb55')[0]['datetime'][:10]\n",
    "    # content 정보 중 좋아요 수 가져오기\n",
    "    try:\n",
    "        like = soup.select('div.Nm9Fw > button')[0].text[4:-1]\n",
    "    except:\n",
    "        like = 0\n",
    "    # content 정보 중 위치정보 가져오기\n",
    "    try:\n",
    "        place = soup.select('div.M30c5')[0].text\n",
    "    except:\n",
    "        place = ' '\n",
    "    \n",
    "    # 뽑아낸 데이터 list에 저장\n",
    "    data = [content, date, like, place, tags]\n",
    "    return data\n",
    "\n",
    "# 다음 게시글로 이동하는 function 정의\n",
    "def move_next(driver):\n",
    "    # 다음 게시글로 넘어가는 화살표 태그로 찾아주기\n",
    "    right = driver.find_element_by_css_selector('a.coreSpriteRightPaginationArrow')\n",
    "    right.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링하여 리스트에 저장한 데이터 엑셀로 저장\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 넣을 프레임 선언\n",
    "result_df = pd.DataFrame(result)\n",
    "# 데이터 컬럼명 설정\n",
    "result_df.columns = ['content', 'data', 'like', 'place', 'tags']\n",
    "# 데이터 저장 타입과 경로 설정\n",
    "result_df.to_excel('crawling_sadang.xlsx')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38332bit349cc2d196a74ae294d57b258faa6317",
   "display_name": "Python 3.8.3 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}